{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation using Noisy Channel\n",
        "\n",
        "Here we will attempt to translate old Irish to new Irish using Noisy Channel.\n",
        "\n",
        "\n",
        "*   We ingest our data and concatenate the words into sentences\n",
        "*   We calculate frequencies of each source word per sentence and repeat for the target words\n",
        "*   We also compute frequencies of a target word being matched by the adjacent word in the target sentence for all occurences of the source word\n",
        "*   Use Bayesian probabilities to predict the next word when translating test data: \n",
        "  **P(new|old) = P(new)P(old|new)**\n",
        "\n",
        "* We process stop words, auxililary verbs and punctuations etc separately as they are more frequent than most words and thus dominate probability calculations\n",
        "\n",
        "  **Limitations**\n",
        "\n",
        "- Cuts off rest of target sentence once end of source sentence reached"
      ],
      "metadata": {
        "id": "qRoj3eoomRYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Files"
      ],
      "metadata": {
        "id": "exV4Mf9_XBhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xDm-dJ0dW7Qv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "path_train_source = r\"/content/train-source.txt\"\n",
        "path_train_target = r\"/content/train-target.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path):\n",
        "\n",
        "  text = []\n",
        "  words = set()\n",
        "\n",
        "  sentences = open(path, 'r', encoding='utf-8')\n",
        "\n",
        "  sent = \"\"\n",
        "  \n",
        "  for ind, word in enumerate(sentences):\n",
        "\n",
        "    token = word.rstrip(\"\\n\").rstrip(\"\\t\").lower()\n",
        "\n",
        "    if token == \"<s>\" or token in [\"'\", '\"']:\n",
        "      # sent = sent\n",
        "      continue\n",
        "\n",
        "    elif token == '</s>':\n",
        "      sent = sent[:-1]\n",
        "      text.append(sent.lstrip())\n",
        "      sent = \"\"\n",
        "\n",
        "    else:\n",
        "      sent = sent + token + \" \"    \n",
        "  \n",
        "    if token not in words:\n",
        "      words.add(token)\n",
        "\n",
        "  word_index = dict([(i, word) for i, word in enumerate(words)])\n",
        "\n",
        "  return text, words, word_index"
      ],
      "metadata": {
        "id": "zJJgmLaqXH-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install unidecode"
      ],
      "metadata": {
        "id": "NekFCfqgDFuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher as SM\n",
        "from unidecode import unidecode as un_dec\n",
        "\n",
        "def compute_frequencies(source, target):\n",
        "\n",
        "  target_frequencies = {}\n",
        "  source_to_target_frequencies = {}\n",
        "  total_st_frequencies = 0\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for x in zip(source, target):\n",
        "\n",
        "    source_words = [word for word in x[0].split(\" \")]\n",
        "    target_words = [word for word in x[1].split(\" \")]\n",
        "\n",
        "    for ind, s_w in enumerate(source_words):\n",
        "\n",
        "      if ind <= len(target_words) - 1:\n",
        "\n",
        "        t_w = target_words[ind]\n",
        "        s_normed = un_dec(s_w)\n",
        "        t_normed = un_dec(t_w)\n",
        "\n",
        "        # compare similarity of the two words\n",
        "        # if not match criteria, completely ignore\n",
        "        if not (SM(None, s_normed, t_normed).ratio() > 0.6 and len(min(s_w,t_w))/len(max(s_w,t_w)) > 0.6):\n",
        "          t_w = compare_words(s_w, t_w, target_words[ind:])\n",
        "          count+=1\n",
        "        else:\n",
        "\n",
        "          # Add target to emission table\n",
        "          if t_w in target_frequencies.keys():\n",
        "            target_frequencies[t_w] += 1\n",
        "          else:\n",
        "            target_frequencies[t_w] = 1\n",
        "\n",
        "          # Add target to transition table for source words\n",
        "          if s_w in source_to_target_frequencies.keys():\n",
        "            if t_w in source_to_target_frequencies[s_w].keys():\n",
        "              source_to_target_frequencies[s_w][t_w] += 1\n",
        "            else:\n",
        "              source_to_target_frequencies[s_w][t_w] = 1\n",
        "\n",
        "          else:\n",
        "            source_to_target_frequencies[s_w] ={}          \n",
        "            source_to_target_frequencies[s_w][t_w] = 1\n",
        "\n",
        "          total_st_frequencies += 1\n",
        "\n",
        "  return target_frequencies, source_to_target_frequencies, total_st_frequencies"
      ],
      "metadata": {
        "id": "ITvjXpS3leP9"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_words(source, target, target_list, ind = -1):\n",
        "\n",
        "  s_normed = un_dec(source)\n",
        "  t_normed = un_dec(target)\n",
        "\n",
        "  ind += 1\n",
        "\n",
        "  if ind > len(target_list)-1:\n",
        "    return \"<PAD>\"\n",
        "\n",
        "  else:\n",
        "\n",
        "    if SM(None, s_normed, t_normed).ratio() > 0.4 and len(min(source, target))/len(max(source, target)) > 0.4:\n",
        "      return target\n",
        "\n",
        "    else:\n",
        "      target = target_list[ind]\n",
        "      return compare_words(source, target, target_list, ind)\n"
      ],
      "metadata": {
        "id": "mA7jaA8dnsVr"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text, input_words, input_word_index = read_file(path_train_source)\n",
        "target_text, target_words, target_word_index = read_file(path_train_target)\n",
        "\n",
        "input_words.add(\"<PAD>\")\n",
        "target_words.add(\"<PAD>\")\n",
        "\n",
        "input_word_index[len(input_word_index)] = \"<PAD>\"\n",
        "target_word_index[len(target_word_index)] = \"<PAD>\"\n"
      ],
      "metadata": {
        "id": "nrb8atNmXMSs"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_frequencies, source_to_target_frequencies, total_st_frequencies = compute_frequencies(input_text, target_text)"
      ],
      "metadata": {
        "id": "FRde5zhglsP6"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noisy Channel Model\n",
        "\n"
      ],
      "metadata": {
        "id": "rQ5ZFXRlXBml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition"
      ],
      "metadata": {
        "id": "OiUG1EnJSOkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install advertools"
      ],
      "metadata": {
        "id": "p5XFTpx9qVXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import advertools as adv\n",
        "def predict(source_sentence, target_words, target_word_index, target_frequencies, source_to_target_frequencies):  \n",
        "\n",
        "  target = []\n",
        "\n",
        "  p_s = [\",\", \"!\", \"-\", \".\", \"?\",\"'\", '\"',\"á\", \"na\", \"liom\", \"leis\", \"léi\", \"'un\", \"<PAD>\"]\n",
        "  stop_punct = list(adv.stopwords[\"irish\"])\n",
        "  stop_punct = stop_punct + p_s\n",
        "\n",
        "  total_freqs = sum(target_frequencies.values())\n",
        "\n",
        "  for ind, val in enumerate(source_sentence):\n",
        "\n",
        "    # if test source word is not in train set, we assume transformed to empty string and skip\n",
        "    if val not in source_to_target_frequencies.keys(): \n",
        "      continue\n",
        "\n",
        "    probs = {}\n",
        "\n",
        "    try:\n",
        "      total_tran_freq = sum(source_to_target_frequencies[val].values())      \n",
        "      for x in source_to_target_frequencies[val].keys():\n",
        "        \n",
        "        # handle punctuation and stop words separately\n",
        "        # they have highest frequencies hence risk of blowing up on prob scores\n",
        "\n",
        "        if (x in stop_punct and val not in stop_punct):\n",
        "          x_prob = 0.0\n",
        "        else:\n",
        "          \n",
        "          # find \"emission\" probabilities\n",
        "          try:\n",
        "            x_prior = target_frequencies[x]/total_freqs\n",
        "          except KeyError:\n",
        "            x_prior = 0.0\n",
        "\n",
        "          # find \"transition\" probabilities\n",
        "          try:\n",
        "            x_post = source_to_target_frequencies[val][x] / total_tran_freq\n",
        "          except KeyError:\n",
        "            x_post = 0.0\n",
        "\n",
        "          x_prob = x_prior * x_post\n",
        "  \n",
        "        probs[x] = x_prob\n",
        "\n",
        "    except KeyError:\n",
        "      x_prob = 0.0\n",
        "      probs[x] = x_prob\n",
        "\n",
        "    new_word = max(probs, key=probs.get)\n",
        "\n",
        "    # equate <PAD> to empty string\n",
        "    if new_word == \"<PAD>\":\n",
        "      continue\n",
        "\n",
        "    target.append(new_word)\n",
        "\n",
        "  return target\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STlhHfjf4H7G"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "Aq1ZXVytSESm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "\n",
        "def evaluate(target_words, target_word_index, target_frequencies, source_to_target_frequencies):\n",
        "\n",
        "  testsource = open('test-source.txt', 'r')\n",
        "  original = []\n",
        "  sentence = []\n",
        "  from_scratch_hypotheses = []\n",
        "  \n",
        "\n",
        "  for line in testsource:\n",
        "    token = line.rstrip(\"\\n\").lower()\n",
        "    if token=='<s>':\n",
        "      sentence = []\n",
        "    elif token == '</s>':\n",
        "      from_scratch_hypotheses.append(predict(sentence,target_words, target_word_index, target_frequencies, source_to_target_frequencies))\n",
        "      original.append(sentence)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "\n",
        "  references = []\n",
        "  testtarget = open('test-target.txt', 'r')\n",
        "\n",
        "  for line in testtarget:\n",
        "    token = line.rstrip(\"\\n\").lower()\n",
        "    if token=='<s>':\n",
        "      sentence = []\n",
        "    elif token == '</s>':\n",
        "      references.append([sentence])\n",
        "    else: \n",
        "      sentence.append(token)\n",
        "\n",
        "  return print((corpus_bleu(references,from_scratch_hypotheses))), from_scratch_hypotheses, references,original"
      ],
      "metadata": {
        "id": "qvBJ7wJnKtxw"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(target_words, target_word_index, target_frequencies, source_to_target_frequencies)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl4r6MT_S-i0",
        "outputId": "78decc18-bda3-4486-b65e-ecdb497b4c8b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7301463927393244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ],
      "metadata": {
        "id": "hSq2r7X_qk7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build N-Grams"
      ],
      "metadata": {
        "id": "4qZoJj8RScrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngrams(input_corpus, target_corpus, len_gram):\n",
        "\n",
        "  ngram_counts = {}\n",
        "  gram_vocab_counts = {}\n",
        "  gram_vocab_counts['<PAD>-<PAD>'] = len_gram - 1\n",
        "\n",
        "  x_ind = 0\n",
        "  for x in zip(input_corpus, target_corpus):\n",
        "    x_ind += 1\n",
        "\n",
        "    source_words = [word for word in x[0].split(\" \")]\n",
        "    target_words = [word for word in x[1].split(\" \")]\n",
        "\n",
        "    # skip sentence pairs whose length varies by a certain length:\n",
        "    if abs(len(source_words)-len(target_words)) > 5:\n",
        "      continue\n",
        "\n",
        "    s_w = \"<PAD>\" #initialize source_word\n",
        "    t_ind = 0 #target index\n",
        "    t_w = target_words[t_ind] #initialize source_word\n",
        "    s_normed = un_dec(source_words[0])\n",
        "    t_normed = un_dec(t_w)\n",
        "\n",
        "    if not (SM(None, s_normed, t_normed).ratio() > 0.4 and len(min(s_w,t_w))/len(max(s_w,t_w)) > 0.4):\n",
        "      t_w = compare_words(s_w, t_w, target_words[0:])\n",
        "    else:\n",
        "      ngram_key = s_w +\"-\" + source_words[0]\n",
        "\n",
        "      # set first target as output word of ngram of <PAD> + first source word\n",
        "      ngram_counts, gram_vocab_counts = update_ngram_counts(ngram_key, t_w, ngram_counts, gram_vocab_counts)\n",
        "\n",
        "      # Sliding through corpus to get ngram counts\n",
        "      for i in range(len(source_words) - (len_gram-1)):\n",
        "        t_ind += 1\n",
        "        i += 1\n",
        "        end = i + len_gram\n",
        "\n",
        "        # if source sentence is longer than target sentence and source index exceeds\n",
        "        # target length, skip target index backwards\n",
        "\n",
        "        if i >= len(target_words):\n",
        "          t_ind = t_ind - 1\n",
        "\n",
        "        # get next source word\n",
        "        s_w = source_words[i]\n",
        "        t_w = target_words[t_ind]\n",
        "\n",
        "        # normalize source and target words and compare similarity\n",
        "        # if not similar, recursively compare next words in the target sentence\n",
        "        s_normed = un_dec(s_w)\n",
        "        t_normed = un_dec(t_w)\n",
        "\n",
        "        if not (SM(None, s_normed, t_normed).ratio() > 0.4 and len(min(s_w,t_w))/len(max(s_w,t_w)) > 0.4):\n",
        "          t_w = compare_words(s_w, t_w, target_words[i:])\n",
        "          t_ind -= 1\n",
        "\n",
        "        # Getting ngram\n",
        "        ngram = [source_words[ind] for ind in range(i-1, i+1)]\n",
        "        ngram_key = \"-\".join(word for word in ngram)\n",
        "\n",
        "        ngram_counts, gram_vocab_counts = update_ngram_counts(ngram_key, t_w, ngram_counts, gram_vocab_counts)\n",
        "\n",
        "\n",
        "  return ngram_counts, gram_vocab_counts, gram_vocab_counts.keys()\n",
        "      "
      ],
      "metadata": {
        "id": "pQsOdEMgqj56"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_ngram_counts(key, t_w, ngram_counts, gram_vocab_counts):\n",
        "\n",
        "  # Keeping track of the ngram counts\n",
        "  if key in ngram_counts.keys():\n",
        "      if t_w in ngram_counts[key]:\n",
        "        ngram_counts[key][t_w] += 1\n",
        "      else:\n",
        "        ngram_counts[key][t_w] = 1\n",
        "  else:\n",
        "      ngram_counts[key] = {}\n",
        "      ngram_counts[key][t_w] = 1\n",
        "\n",
        "  # Track vocab counts\n",
        "  if key in gram_vocab_counts.keys():\n",
        "    gram_vocab_counts[key] += 1\n",
        "  else:\n",
        "    gram_vocab_counts[key] = 1\n",
        "\n",
        "  return ngram_counts, gram_vocab_counts\n"
      ],
      "metadata": {
        "id": "hC_ZDKE-mMN_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_to_target_counts, ngram_vocab_counts, ngram_vocab = build_ngrams(input_text, target_text, 2)"
      ],
      "metadata": {
        "id": "cJJ_MUmBxQL2"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition"
      ],
      "metadata": {
        "id": "JhZkPSrmShau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_next(input, ngram_counts, vocab_counts, vocab, target_words, len_gram,  laplace_weight=0):\n",
        "\n",
        "  target = []\n",
        "\n",
        "  p_s = [\",\", \"!\", \"-\", \".\", \"?\",\"'\", '\"',\"á\", \"na\", \"liom\", \"leis\", \"léi\", \"'un\", \"<PAD>\"]\n",
        "  stop_punct = list(adv.stopwords[\"irish\"])\n",
        "  stop_punct = stop_punct + p_s\n",
        "\n",
        "\n",
        "  if isinstance(input, str):\n",
        "    source_sentence = input.split(\" \")\n",
        "  else: \n",
        "    source_sentence = input\n",
        "\n",
        "  total_freqs = sum(vocab_counts.values())\n",
        "\n",
        "  pad_str = \"<PAD>-\" * (len_gram-1)\n",
        "  s_w = source_sentence[0]\n",
        "  source_gram = pad_str + s_w \n",
        "\n",
        "  if source_gram in ngram_counts.keys():\n",
        "\n",
        "    vocab_probabilities = {}\n",
        "    total_tran_freq = sum(ngram_counts[source_gram].values())\n",
        "\n",
        "    for x in ngram_counts[source_gram].keys():\n",
        "\n",
        "      if (x in stop_punct and s_w not in stop_punct):\n",
        "        x_prob = 0.0\n",
        "      else:\n",
        "        x_prob = calc_probs(vocab_counts[source_gram], total_freqs, ngram_counts[source_gram][x], total_tran_freq)    \n",
        "\n",
        "      vocab_probabilities[x] = x_prob\n",
        "\n",
        "    new_word = max(vocab_probabilities, key=vocab_probabilities.get)\n",
        "\n",
        "    if new_word != \"<PAD>\":\n",
        "      target.append(new_word)\n",
        "\n",
        "    \n",
        "\n",
        "  for ind, val in enumerate(source_sentence):\n",
        "\n",
        "    if ind < len(source_sentence) - 1:\n",
        "      vocab_probabilities = {}\n",
        "      source_gram = source_sentence[ind] + \"-\" + source_sentence[ind+1]\n",
        "\n",
        "      # if test source ngram is not in train set, we assume transformed to empty string and skip\n",
        "      if source_gram not in ngram_counts.keys():\n",
        "        continue\n",
        "\n",
        "      total_tran_freq = sum(ngram_counts[source_gram].values())\n",
        "\n",
        "      for x in ngram_counts[source_gram].keys():\n",
        "\n",
        "        if (x in stop_punct and s_w not in stop_punct):\n",
        "          x_prob = 0.0\n",
        "        else:\n",
        "          x_prob = calc_probs(vocab_counts[source_gram], total_freqs, ngram_counts[source_gram][x], total_tran_freq)    \n",
        "\n",
        "        vocab_probabilities[x] = x_prob\n",
        "\n",
        "      new_word = max(vocab_probabilities, key=vocab_probabilities.get)\n",
        "\n",
        "      if new_word != \"<PAD>\":\n",
        "        target.append(new_word)\n",
        "\n",
        "\n",
        "  return target"
      ],
      "metadata": {
        "id": "Z7GrgqQxKRYa"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_probs(ngram_freq, total_freqs, word_tran_freq, total_tran_freq):\n",
        "\n",
        "  # find \"emission\" probabilities\n",
        "  try:\n",
        "    x_prior = ngram_freq / total_freqs\n",
        "\n",
        "  except KeyError:\n",
        "    x_prior = 0.0\n",
        "\n",
        "  # find \"transition\" probabilities\n",
        "  try:\n",
        "    x_post = word_tran_freq / total_tran_freq\n",
        "  except KeyError:\n",
        "    x_post = 0.0\n",
        "\n",
        "  x_prob = x_prior * x_post\n",
        "\n",
        "  return x_prob"
      ],
      "metadata": {
        "id": "hA7XW5ITKhZ2"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "\n",
        "def evaluate_alt(target_words, ngram_to_target_counts, ngram_vocab_counts, ngram_vocab):\n",
        "\n",
        "  testsource = open('test-source.txt', 'r')\n",
        "  original = []\n",
        "  sentence = []\n",
        "  from_scratch_hypotheses = []\n",
        "  \n",
        "\n",
        "  for line in testsource:\n",
        "    token = line.rstrip(\"\\n\").lower()\n",
        "\n",
        "    if token=='<s>':\n",
        "      sentence = []\n",
        "\n",
        "    elif token == '</s>':\n",
        "      from_scratch_hypotheses.append(suggest_next(sentence,\n",
        "                                                  ngram_to_target_counts,\n",
        "                                                  ngram_vocab_counts, \n",
        "                                                  ngram_vocab, \n",
        "                                                  target_words, \n",
        "                                                  len_gram=2))\n",
        "      original.append(sentence)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "\n",
        "  references = []\n",
        "\n",
        "  testtarget = open('test-target.txt', 'r')\n",
        "\n",
        "  for line in testtarget:\n",
        "    token = line.rstrip(\"\\n\").lower()\n",
        "    if token=='<s>':\n",
        "      sentence = []\n",
        "    elif token == '</s>':\n",
        "      references.append([sentence])\n",
        "    else: \n",
        "      sentence.append(token)\n",
        "      \n",
        "  return print((corpus_bleu(references,from_scratch_hypotheses))), from_scratch_hypotheses, references,original"
      ],
      "metadata": {
        "id": "WFyT3K_qRvPf"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, pred, ref, original = evaluate_alt(target_words, ngram_to_target_counts, ngram_vocab_counts, ngram_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcbaNAr2SoWe",
        "outputId": "4a79cc22-4bce-4b56-f5a6-63979655806f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30349414948351977\n"
          ]
        }
      ]
    }
  ]
}